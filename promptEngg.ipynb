{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keyom-ai/prompts/blob/main/promptEngg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyTfGXPo1P_m"
      },
      "source": [
        "**Prompt Engineering: Crafting Inputs for Language Models**\n",
        "\n",
        "**Definition**\n",
        "Prompt engineering is the process of designing and refining the input prompts used to interact with natural language processing (NLP) models, particularly those like GPT (Generative Pre-trained Transformer).\n",
        "\n",
        "**Objective**\n",
        "The primary goal of prompt engineering is to generate desired and specific responses from the language model by carefully crafting the input it receives.\n",
        "\n",
        "**GPT Models and Prompt Engineering**\n",
        "In the context of GPT models, users provide a prompt or input text, and the model generates a continuation or completion based on its training data. Prompt engineering involves experimenting with different forms of prompts, instructions, or queries to elicit the desired information or response.\n",
        "\n",
        "**Key Aspects of Prompt Engineering**\n",
        "**Effective prompt engineering requires a combination of:**\n",
        "\n",
        "**Domain Knowledge:** Understanding the subject matter to create relevant prompts.\n",
        "\n",
        "**Model Understanding:** Awareness of the model's capabilities and limitations.\n",
        "\n",
        "**Iterative Testing:** Experimenting with different prompt variations to find the most effective ones for a given task or application.\n",
        "\n",
        "**Considerations**\n",
        "While prompt engineering can enhance the performance of language models, it may not completely eliminate biases or ensure perfect responses. Interpretation and critical evaluation of model outputs are crucial, and ethical considerations should be taken into account when deploying language models in real-world applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PGypSsX1euy"
      },
      "source": [
        "______________________\n",
        "**Language Models: Understanding LLMs**\n",
        "\n",
        "**Definition**\n",
        "LLMs, or Language Models, are computational models designed to understand and generate human-like language. These models, often based on deep learning architectures, aim to comprehend the structure, context, and semantics of natural language.\n",
        "\n",
        "**Types of LLMs**\n",
        "There are various types of language models, with one prominent example being the GPT (Generative Pre-trained Transformer) series. LLMs can be categorized based on their architecture, training data, and specific applications.\n",
        "\n",
        "**Functionality**\n",
        "LLMs process input data, typically in the form of text, and generate coherent and contextually relevant output. They are capable of tasks such as text completion, translation, summarization, and question answering.\n",
        "\n",
        "**Pre-training and Fine-tuning**\n",
        "Most LLMs undergo a two-step process: pre-training and fine-tuning. In the pre-training phase, models learn language patterns from vast datasets. Fine-tuning is then performed on domain-specific data to tailor the model for particular tasks or industries.\n",
        "\n",
        "**Role of Prompt Engineering in LLMs**\n",
        "Prompt engineering plays a crucial role when interacting with LLMs. Users provide prompts or queries to elicit specific responses from the model. Crafting effective prompts involves experimenting with language, structure, and format to guide the LLM toward the desired output.\n",
        "\n",
        "**Considerations and Challenges**\n",
        "Despite their capabilities, LLMs may exhibit biases present in their training data and may not always produce accurate or unbiased results. Continuous research and ethical considerations are necessary to address these challenges.\n",
        "\n",
        "**Applications**\n",
        "LLMs find applications in a wide range of fields, including natural language understanding, content generation, chatbots, and more. Their versatility makes them valuable tools for automating various language-related tasks.\n",
        "\n",
        "**Future Developments**\n",
        "Ongoing research in the field of LLMs aims to improve their understanding of context, enhance interpretability, and mitigate biases. As advancements continue, LLMs are likely to play an increasingly integral role in natural language processing and human-computer interactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-BrXJTd1rua"
      },
      "source": [
        "___________________\n",
        "**Language Models (LLMs) and Prompt Engineering: Exploring Similarities and Differences**\n",
        "\n",
        "**Similarities**\n",
        "\n",
        "1. **Natural Language Processing (NLP):**\n",
        "   - Both LLMs and prompt engineering are integral components of natural language processing. LLMs are designed to understand and generate human-like language, while prompt engineering focuses on crafting input to elicit desired responses from these models.\n",
        "\n",
        "2. **Task Customization:**\n",
        "   - Both concepts involve customization for specific tasks. LLMs undergo fine-tuning to adapt to particular domains or applications, and prompt engineering tailors inputs to achieve specific outcomes.\n",
        "\n",
        "3. **Iterative Optimization:**\n",
        "   - Both LLMs and prompt engineering often require an iterative optimization process. Fine-tuning LLMs and experimenting with different prompts are essential for enhancing performance and achieving desired results.\n",
        "\n",
        "**Differences**\n",
        "\n",
        "1. **Model vs. Input Focus:**\n",
        "   - LLMs primarily focus on the model itself, involving architecture, training data, and understanding language patterns. Prompt engineering, on the other hand, centers around optimizing the input provided to the model to guide its output.\n",
        "\n",
        "2. **Training Phases:**\n",
        "   - LLMs typically undergo two main phases: pre-training on large datasets to learn language patterns and fine-tuning on specific data for task adaptation. Prompt engineering is part of the interaction process with a pre-trained model and doesn't involve training phases.\n",
        "\n",
        "3. **Scope of Influence:**\n",
        "   - LLMs have a broader scope, encompassing various language-related tasks such as text generation, translation, summarization, etc. Prompt engineering is more task-specific, focusing on obtaining desired outcomes for a particular interaction or query.\n",
        "\n",
        "4. **Addressing Biases:**\n",
        "   - Prompt engineering is often used to mitigate biases in model outputs by carefully choosing and structuring inputs. While LLMs can inherit biases from their training data, addressing and mitigating biases in the model itself is a distinct challenge.\n",
        "\n",
        "5. **Versatility and Applicability:**\n",
        "   - LLMs are versatile tools applicable to a wide range of language-related tasks. Prompt engineering is a technique specifically used in the context of interacting with language models, enhancing their usability for specific applications.\n",
        "\n",
        "**Interdependence and Synergy**\n",
        "Despite their differences, LLMs and prompt engineering are often interdependent and can work synergistically. Effective prompt engineering can maximize the utility of LLMs by guiding them to produce more accurate and relevant outputs for specific tasks. Understanding the interplay between these concepts is crucial for harnessing the full potential of language models in various applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR2wDKPKqdKI"
      },
      "source": [
        "________________________________________________________________________________\n",
        "**Let's engage in some hands-on prompt engineering.**\n",
        "\n",
        "**Let’s Set up the API key and install dependencies:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_prnZV7sDB1K",
        "outputId": "b6dfffed-7a1b-425e-f532-61da08cef457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ],
      "source": [
        "pip install openai==0.28 #Install OpenAI Python Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "v0HC_TViDGpN"
      },
      "outputs": [],
      "source": [
        "import openai #Import the OpenAI Package in Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mPYWz0VwDJeO"
      },
      "outputs": [],
      "source": [
        "import os #Import the os Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rd0sjV8QDMFe"
      },
      "outputs": [],
      "source": [
        "import json # Import the 'json' module in Python for encoding and decoding JSON data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAIF98R0DO1n",
        "outputId": "0feaf0b5-27bd-42a7-9770-b985a50ae314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diff-match-patch in /usr/local/lib/python3.10/dist-packages (20230430)\n"
          ]
        }
      ],
      "source": [
        "pip install diff-match-patch # Use this command to install the diff-match-patch library via pip. # This library is useful for comparing and manipulating text differences, # making it handy for tasks like text comparison, version control, and data synchronization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uUjcY6KDSNd",
        "outputId": "64482944-32a2-4035-b493-041c255bf24d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: openai\n",
            "Version: 0.28.0\n",
            "Summary: Python client library for the OpenAI API\n",
            "Home-page: https://github.com/openai/openai-python\n",
            "Author: OpenAI\n",
            "Author-email: support@openai.com\n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, requests, tqdm\n",
            "Required-by: llmx\n"
          ]
        }
      ],
      "source": [
        "pip show openai #Display Information about the OpenAI Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eB-ma-QDWf-",
        "outputId": "314136ec-3e42-4d87-97f2-0c430051ab76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: redlines in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from redlines) (8.1.7)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.3.5 in /usr/local/lib/python3.10/dist-packages (from redlines) (13.7.0)\n",
            "Requirement already satisfied: rich-click<2.0.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from redlines) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.3.5->redlines) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.3.5->redlines) (2.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from rich-click<2.0.0,>=1.6.1->redlines) (4.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.3.5->redlines) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install redlines # Install the 'redlines' Python package using pip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "OOrmW-NEDdRW",
        "outputId": "d578dcc4-cdec-40f2-f9ff-a521dff33914"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-eb5ec30a3d99>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Set OpenAI API Key as a Variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your API Key: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "#Set OpenAI API Key as a Variable\n",
        "openai.api_key = input(\"Enter your API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELd86uk6tVX-"
      },
      "source": [
        "________________________________________________________________________________\n",
        "**Below Helper Function will be used to get the output from the ChatGPT.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nGagchBFDew9"
      },
      "outputs": [],
      "source": [
        "#Get the output from the ChatGPT\n",
        "#Import the OpenAI Module\n",
        "import openai\n",
        "\n",
        "#Define a Function for Text Completion\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "\n",
        "    \"\"\"\n",
        "    This function takes a prompt as input and uses OpenAI's Chat Completion API\n",
        "    to generate a model completion. It returns the generated completion.\n",
        "\n",
        "    Parameters:\n",
        "    - prompt: The input text prompt for the model.\n",
        "    - model: The OpenAI model to use (default is \"gpt-3.5-turbo\").\n",
        "    - api_key: The OpenAI API key for authentication (replace \"YOUR_API_KEY\" with the actual key).\n",
        "\n",
        "    Returns:\n",
        "    - The generated completion from the model.\n",
        "    \"\"\"\n",
        "\n",
        "    #Create Messages for Chat Completion\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    #Make a Chat Completion Request to OpenAI API\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0  # this is the degree of randomness of the model's output\n",
        "    )\n",
        "\n",
        "    #6. Extract and Return the Model's Completion\n",
        "    return response['choices'][0]['message']['content']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hdyg8eLtcXk"
      },
      "source": [
        "________________________________________________________________________________\n",
        "**Tactic 1:** Use delimiters to clearly indicate distinct parts of the input:\n",
        "Delimiters can be anything like: ```, “””, < >, <tag> </tag>, :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ejr5u6SmDjnV",
        "outputId": "64805152-c18c-4d67-8ffa-d186968b309e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crafting effective prompts involves conveying expectations clearly and precisely, providing explicit instructions to guide the model towards the desired output while avoiding irrelevant or inaccurate responses, emphasizing the importance of clarity over brevity, as longer prompts can enhance clarity and context, resulting in more intricate and contextually relevant outputs.\n"
          ]
        }
      ],
      "source": [
        "#Tactic 1 Use delimiters\n",
        "\n",
        "# Example Usage with Provided Text\n",
        "#Create a formatted text using the provided content.\n",
        "text = f\"\"\"\n",
        "Crafting effective prompts involves conveying your expectations\n",
        "with utmost clarity and precision. Offering instructions that\n",
        "are as explicit as possible serves as a roadmap, directing the\n",
        "model towards the intended output while minimizing the risk of\n",
        "irrelevant or inaccurate responses. It's crucial not to conflate\n",
        "a clear prompt with a concise one—while brevity has its merits,\n",
        "the primary focus should be on clarity. In numerous scenarios,\n",
        "lengthier prompts contribute to enhanced clarity and context,\n",
        "enabling the model to produce more intricate and contextually\n",
        "relevant outputs.\n",
        "\"\"\"\n",
        "\n",
        "# Create a prompt for the get_completion function with the formatted text enclosed in triple backticks.\n",
        "prompt = f\"\"\"\n",
        "Summarize the text delimited by triple backticks \\\n",
        "into a single sentence.\n",
        "````{text}````  # Use four backticks to represent triple backticks and escape the content within.\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion from the model.\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJo5ePiCtv3d"
      },
      "source": [
        "________________________________________________________________________________\n",
        "**Tactic 2:** Ask for a structured output\n",
        "JSON, HTML\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2a517EEJDrGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe6f3b8-6e0b-4c19-8378-c9b06a70b3cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"book_id\": 1,\n",
            "    \"title\": \"The Enigmatic Echo\",\n",
            "    \"author\": \"Sylvia Mystique\",\n",
            "    \"genre\": \"Mystery\"\n",
            "  },\n",
            "  {\n",
            "    \"book_id\": 2,\n",
            "    \"title\": \"Chronicles of Nebula\",\n",
            "    \"author\": \"Xander Celestia\",\n",
            "    \"genre\": \"Science Fiction\"\n",
            "  },\n",
            "  {\n",
            "    \"book_id\": 3,\n",
            "    \"title\": \"Whispers in the Woods\",\n",
            "    \"author\": \"Elena Shadowborn\",\n",
            "    \"genre\": \"Fantasy\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "#Tactic 2:\n",
        "#JSON, HTML\n",
        "\n",
        "# Import the json module for handling JSON data\n",
        "import json\n",
        "\n",
        "#Define the prompt as a multi-line string using triple quotes.\n",
        "prompt = \"\"\"\n",
        "Generate a list of three made-up book titles along\n",
        "with their authors and genres.\n",
        "Provide them in JSON format with the following keys:\n",
        "book_id, title, author, genre.\n",
        "\"\"\"\n",
        "\n",
        "#Create a list of dictionaries, each representing a book with book_id, title, author, and genre.\n",
        "books = [\n",
        "    {\"book_id\": 1, \"title\": \"The Enigmatic Echo\", \"author\": \"Sylvia Mystique\", \"genre\": \"Mystery\"},\n",
        "    {\"book_id\": 2, \"title\": \"Chronicles of Nebula\", \"author\": \"Xander Celestia\", \"genre\": \"Science Fiction\"},\n",
        "    {\"book_id\": 3, \"title\": \"Whispers in the Woods\", \"author\": \"Elena Shadowborn\", \"genre\": \"Fantasy\"}\n",
        "]\n",
        "\n",
        "#Convert the list of dictionaries to JSON format using json.dumps().\n",
        "json_books = json.dumps(books, indent=2)  # indent for pretty formatting\n",
        "\n",
        "#Print the JSON representation of the books.\n",
        "print(json_books)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW_TQb6ot0m7"
      },
      "source": [
        "________________________________________________________________________________\n",
        "**Tactic 3: Ask the model to check whether conditions are satisfied**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ULbzO1oyDtGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c83f1325-bcd2-425f-f751-4f8c09416058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1 - Set some water to boil.\n",
            "Step 2 - Fetch a mug and place a coffee filter in it.\n",
            "Step 3 - When the water reaches the desired temperature, pour it over the coffee grounds.\n",
            "Step 4 - Allow it a moment to steep.\n",
            "Step 5 - Remove the filter.\n",
            "Step 6 - Customize your brew with sugar or a splash of milk, if desired.\n",
            "Step 7 - Your perfect cup of coffee is ready to savor.\n"
          ]
        }
      ],
      "source": [
        "#Tactic 3: Ask the model to check whether conditions are satisfied\n",
        "\n",
        "#Define the input text\n",
        "\n",
        "text_1 = f\"\"\"\n",
        "Whipping up a delightful cup of coffee is a breeze! Start by setting\n",
        "some water to boil. Meanwhile, fetch a mug and place a coffee filter\n",
        "in it. When the water reaches the desired temperature, simply pour it\n",
        "over the coffee grounds. Allow it a moment to steep, letting the rich\n",
        "coffee flavor unfold. After a few minutes, remove the filter, and if\n",
        "you prefer, customize your brew with sugar or a splash of milk. Voila!\n",
        "Your perfect cup of coffee is ready to savor.\n",
        "\"\"\"\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You will be provided with text delimited by triple quotes.\n",
        "If it contains a sequence of instructions, \\\n",
        "re-write those instructions in the following format:\n",
        "\n",
        "Step 1 - ...\n",
        "Step 2 - …\n",
        "…\n",
        "Step N - …\n",
        "\n",
        "If the text does not contain a sequence of instructions, \\\n",
        "then simply write \\\"No steps provided.\\\"\n",
        "\n",
        "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "#Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "#Print the pre-defined generated JSON.\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAoygr0ft_TE"
      },
      "source": [
        "________________________________________________________________________________\n",
        "**Tactic 3: Example 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YATFp3CaD2Su",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df0ea67-4b7b-4dd5-a16b-ab139974bbb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion for Text 2:\n",
            "No steps provided.\n"
          ]
        }
      ],
      "source": [
        "#Tactic 3: Example 2\n",
        "\n",
        "# Define the input text\n",
        "\n",
        "text_2 = \"\"\"\n",
        "The moon is casting its gentle glow tonight, and a symphony of\n",
        "crickets fills the air. It's a serene evening, ideal for a\n",
        "leisurely stroll under the starlit sky. The night blossoms with\n",
        "tranquility as the nocturnal world comes alive. The soft rustle of\n",
        "leaves accompanies the rhythmic chirping of crickets, creating a\n",
        "peaceful ambiance. People are embracing the quietude, some engrossed\n",
        "in quiet conversations, while others find solace in the stillness.\n",
        "It's a perfect night to unwind outdoors and immerse oneself in the\n",
        "tranquil beauty of the nocturnal world.\n",
        "\"\"\"\n",
        "\n",
        "# Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You will be provided with text delimited by triple quotes.\n",
        "If it contains a sequence of instructions,\n",
        "re-write those instructions in the following format:\n",
        "Step 1 - ...\n",
        "Step 2 - …\n",
        "…\n",
        "Step N - …\n",
        "\n",
        "If the text does not contain a sequence of instructions,\n",
        "then simply write \"No steps provided.\"\n",
        "\n",
        "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the header indicating the completion is for Text 2\n",
        "print(\"Completion for Text 2:\")\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz211sgRuDZ0"
      },
      "source": [
        "________________________________________________________________________________\n",
        "**Tactic 4:** “Few-shot” prompting.\n",
        "\n",
        "This tactic is a great practice to do our job very efficiently with chatgpt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YeEYJa7cD4Pu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3629d8c-f1d5-4f04-8907-abb129ab95dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<grandparent>: Knowledge, my dear, is the treasure trove of information and understanding that we gather throughout our lives. It is the key that unlocks the doors of curiosity and empowers us to explore the world around us. Like a wellspring of wisdom, knowledge enriches our minds and enables us to make informed choices and navigate the complexities of life.\n"
          ]
        }
      ],
      "source": [
        "#Tactic 4: “Few-shot” prompting\n",
        "\n",
        "# Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your task is to answer in a consistent style.\n",
        "\n",
        "<child>: Teach me about common sense.\n",
        "\n",
        "<grandparent>: Common sense, dear one, is like the compass guiding a ship\n",
        "through turbulent seas; it's the foundation upon which the sturdiest bridges\n",
        "of understanding are built. Just as the simplest puzzle pieces form the\n",
        "clearest picture, common sense weaves the fabric of sound decision-making\n",
        "and thoughtful reasoning.\n",
        "\n",
        "<child>: Teach me about knowledge.\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQPvJ0fhuLMM"
      },
      "source": [
        "_________\n",
        "**Tactic 5:** Specify the steps required to complete a task.\n",
        "\n",
        "This is how we can make chatgpt to work for us step by step\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dDBERhM9D_6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf0b6e2b-aa4e-442b-ab52-caada33a9c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion for prompt 1:\n",
            "1 - Mia and Ethan, cousins, gather apples from an orchard but encounter a misfortune when Mia slips and Ethan falls while trying to help her, but they continue their adventure undeterred.\n",
            "\n",
            "2 - Mia et Ethan, cousins, récoltent des pommes dans un verger mais rencontrent un malheur lorsque Mia glisse et qu'Ethan tombe en essayant de l'aider, mais ils continuent leur aventure sans se décourager.\n",
            "\n",
            "3 - Mia, Ethan\n",
            "\n",
            "4 - {\n",
            "  \"french_summary\": \"Mia et Ethan, cousins, récoltent des pommes dans un verger mais rencontrent un malheur lorsque Mia glisse et qu'Ethan tombe en essayant de l'aider, mais ils continuent leur aventure sans se décourager.\",\n",
            "  \"num_names\": 2\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "#Tactic 5:\n",
        "\n",
        "# Define the input text\n",
        "text = f\"\"\"\n",
        "In a quaint town, cousins Mia and Ethan embarked on an exciting\n",
        "mission to gather apples from an orchard atop a gentle slope.\n",
        "As they ascended, laughter filled the air, echoing their shared\n",
        "enthusiasm. Suddenly, a moment of misfortune unfolded—Mia slipped\n",
        "on a loose patch of soil, and Ethan, trying to assist, joined in\n",
        "the descent. Despite a few scratches, the duo made their way back\n",
        "to the town, where warm embraces awaited. Undeterred by the incident,\n",
        "their adventurous zeal persisted, and they eagerly ventured forth to\n",
        "discover more hidden treasures.\n",
        "\"\"\"\n",
        "\n",
        "# Construct the prompt with the provided text\n",
        "prompt_1 = f\"\"\"\n",
        "Perform the following actions:\n",
        "1 - Summarize the following text delimited by triple backticks with 1 sentence.\n",
        "2 - Translate the summary into French.\n",
        "3 - List each name in the French summary.\n",
        "4 - Output a json object that contains the following keys: french_summary, num_names.\n",
        "\n",
        "Separate your answers with line breaks.\n",
        "\n",
        "Text:\n",
        "\n",
        "'''{text}'''\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt_1)\n",
        "\n",
        "# Print the header indicating the completion is for prompt 1\n",
        "print(\"Completion for prompt 1:\")\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGg1wvwCufKZ"
      },
      "source": [
        "_____________\n",
        "**Tactic 5: Another prompt on the same text**\n",
        "\n",
        "This is how we can make chatgpt to work for us step by step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OplWR8ipEIs3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19178882-e0d5-4b9e-c259-e1b8616f69d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion for prompt 2:\n",
            "Summary: Sam and Emily, two brothers, go on an adventure to collect wildflowers but encounter a mishap when Sam stumbles on a hidden root and Emily joins in a playful descent down the slope.\n",
            "\n",
            "Translation: Sam et Emily, deux frères, partent à l'aventure pour collecter des fleurs sauvages mais rencontrent un incident lorsque Sam trébuche sur une racine cachée et qu'Emily se joint à une descente enjouée de la pente.\n",
            "\n",
            "Names: Sam, Emily\n",
            "\n",
            "Output JSON: {\"french_summary\": \"Sam et Emily, deux frères, partent à l'aventure pour collecter des fleurs sauvages mais rencontrent un incident lorsque Sam trébuche sur une racine cachée et qu'Emily se joint à une descente enjouée de la pente.\", \"num_names\": 2}\n"
          ]
        }
      ],
      "source": [
        "#Tactic 5: Another prompt on the same text\n",
        "\n",
        "# Define the input text\n",
        "text = f\"\"\"\n",
        "In a picturesque hamlet, brothers Sam and Emily embarked on a mission\n",
        "to retrieve wildflowers from a hillside meadow. Ascending the slope,\n",
        "their cheerful voices harmonized with the rustling leaves. Fate took\n",
        "an unexpected turn—Sam stumbled on a hidden root, and Emily, trying to\n",
        "help, joined in a playful descent down the slope. Despite a few minor\n",
        "bruises, the siblings made their way back home, where warm embraces\n",
        "awaited them. Undeterred by the little mishap, their adventurous hearts\n",
        "remained unscathed, and they eagerly resumed their exploration\n",
        "with boundless delight.\n",
        "\"\"\"\n",
        "\n",
        "# Construct the prompt with the provided text\n",
        "\n",
        "prompt_2 = f\"\"\"\n",
        "Your task is to perform the following actions:\n",
        "1 - Summarize the following text delimited by\n",
        "  <> with 1 sentence.\n",
        "2 - Translate the summary into French.\n",
        "3 - List each name in the French summary.\n",
        "4 - Output a json object that contains the\n",
        "  following keys: french_summary, num_names.\n",
        "\n",
        "Use the following format:\n",
        "Text: <text to summarize>\n",
        "Summary: <summary>\n",
        "Translation: <summary translation>\n",
        "Names: <list of names in Italian summary>\n",
        "Output JSON: <json with summary and num_names>\n",
        "\n",
        "Text: <{text}>\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt_2)\n",
        "\n",
        "# Print the header indicating the completion is for prompt 1\n",
        "print(\"Completion for prompt 2:\")\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIvytmpvujVp"
      },
      "source": [
        "__________\n",
        "**Tactic 6: Instruct the model to work out its own solution before rushing to a conclusion**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YgD2TxMBENT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b940a8b0-e126-4ff0-f12f-afba54260080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The student's calculation is correct. The total cost for the first year of operations is indeed 480x + 100,000.\n"
          ]
        }
      ],
      "source": [
        "#Tactic 6:\n",
        "\n",
        "# Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Determine if the student's calculation is correct or not.\n",
        "\n",
        "Question:\n",
        "I'm building a solar power installation and I need \\\n",
        " help working out the financials.\n",
        "- Land costs $120 / square foot\n",
        "- I can buy solar panels for $260 / square foot\n",
        "- I negotiated a contract for maintenance that will cost \\\n",
        "me a flat $100k per year, and an additional $10 / square \\\n",
        "foot\n",
        "What is the total cost for the first year of operations\n",
        "as a function of the number of square feet.\n",
        "\n",
        "Student's Solution:\n",
        "Let x be the size of the installation in square feet.\n",
        "Costs:\n",
        "1. Land cost: 120x\n",
        "2. Solar panel cost: 260x\n",
        "3. Maintenance cost: 100,000 + 100x\n",
        "Total cost: 120x + 260x + 100,000 + 100x = 480x + 100,000\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyi65-1Juply"
      },
      "source": [
        "____________\n",
        "**Note that the student’s solution is actually not correct.**\n",
        "\n",
        "*We* can fix this by instructing the model to work out its own solution first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "HO_h_idwEWv7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6082bf19-488e-42e2-e159-e893c59141cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To calculate the total cost for the first year of operations, we need to add up the costs of land, solar panels, and maintenance.\n",
            "\n",
            "Let x be the size of the installation in square feet.\n",
            "\n",
            "Land cost: $120 * x\n",
            "Solar panel cost: $260 * x\n",
            "Maintenance cost: $100,000 + $10 * x\n",
            "\n",
            "Total cost: $120 * x + $260 * x + $100,000 + $10 * x = $390 * x + $100,000\n",
            "\n",
            "Is the student's solution the same as the actual solution just calculated:\n",
            "Yes\n",
            "\n",
            "Student grade:\n",
            "Correct\n"
          ]
        }
      ],
      "source": [
        "#Tactic 6: Note that the student’s solution is actually not correct.\n",
        "\n",
        "# Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your task is to determine if the student's solution \\\n",
        "is correct or not.\n",
        "To solve the problem do the following:\n",
        "- First, work out your own solution to the problem.\n",
        "- Then compare your solution to the student's solution \\\n",
        "and evaluate if the student's solution is correct or not.\n",
        "Don't decide if the student's solution is correct until\n",
        "you have done the problem yourself.\n",
        "\n",
        "Use the following format:\n",
        "Question:\n",
        "\n",
        "```\n",
        "question here\n",
        "```\n",
        "Student's solution:\n",
        "```\n",
        "student's solution here\n",
        "```\n",
        "Actual solution:\n",
        "```\n",
        "steps to work out the solution and your solution here\n",
        "```\n",
        "Is the student's solution the same as actual solution \\\n",
        "just calculated:\n",
        "```\n",
        "yes or no\n",
        "```\n",
        "Student grade:\n",
        "```\n",
        "correct or incorrect\n",
        "```\n",
        "\n",
        "Question:\n",
        "```\n",
        "I'm building a solar power installation and I need help\n",
        "working out the financials.\n",
        "\n",
        "Land costs $120 / square foot\n",
        "I can buy solar panels for $260 / square foot\n",
        "I negotiated a contract for maintenance that will cost\n",
        "me a flat $100k per year, and an additional $10 / square\n",
        "foot\n",
        "What is the total cost for the first year of operations\n",
        "as a function of the number of square feet.\n",
        "```\n",
        "Student's solution:\n",
        "```\n",
        "Let x be the size of the installation in square feet.\n",
        "Costs:\n",
        "\n",
        "Land cost: 110x\n",
        "Solar panel cost: 260x\n",
        "Maintenance cost: 100,000 + 100x\n",
        "Total cost: 120x + 260x + 100,000 + 100x = 480x + 100,000\n",
        "```\n",
        "Actual solution:\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZIQXpvTvHbY"
      },
      "source": [
        "______________\n",
        "**Iterative Prompt Development:**\n",
        "\n",
        "We’ll iteratively analyze and refine our prompts to generate marketing copy from a product fact sheet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "GJZhm9GbEcaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348a564b-7307-4141-bdf3-a8feb5ca3dda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Introducing our versatile and sleek table, a perfect addition to modern office spaces. With options for tabletop finish, leg style, and color choices, it can be customized to suit any workspace. Its sturdy metal frame and adjustable leg levelers ensure durability and stability. Available in various sizes, it is suitable for both collaborative workspaces and individual offices. Ideal for home offices or corporate environments. Designed and manufactured in Germany.\n"
          ]
        }
      ],
      "source": [
        "#Iterative Prompt Development:\n",
        "\n",
        "# Define the input text\n",
        "\n",
        "fact_sheet_table = \"\"\"\n",
        "OVERVIEW\n",
        "- A versatile addition to our contemporary furniture collection, this table complements\n",
        "modern office spaces with its sleek design.\n",
        "- Part of a comprehensive range that includes desks, storage units, and ergonomic chairs.\n",
        "- Options for tabletop finish, leg style, and color choices.\n",
        "- Available in various sizes to accommodate different workspace needs.\n",
        "- Suitable for both collaborative workspaces and individual offices.\n",
        "- Ideal for home offices or corporate environments.\n",
        "\n",
        "CONSTRUCTION\n",
        "- Sturdy metal frame with a powder-coated finish ensures durability.\n",
        "- Adjustable leg levelers for stability on uneven surfaces.\n",
        "\n",
        "DIMENSIONS\n",
        "- WIDTH 120 CM | 47.24”\n",
        "- DEPTH 80 CM | 31.50”\n",
        "- HEIGHT 75 CM | 29.53”\n",
        "\n",
        "OPTIONS\n",
        "- Tabletop finish options: laminate, veneer, or glass.\n",
        "- Leg styles: straight or angled.\n",
        "- Multiple color choices for both the tabletop and legs.\n",
        "\n",
        "MATERIALS\n",
        "FRAME\n",
        "- Powder-coated steel\n",
        "TABLETOP\n",
        "- Choice of high-quality laminate, veneer, or glass.\n",
        "\n",
        "COUNTRY OF ORIGIN\n",
        "- Designed and manufactured in Germany\n",
        "\"\"\"\n",
        "\n",
        "# Construct the prompt with the provided text\n",
        "\n",
        "#  Limit the number of words/sentences/characters.\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your task is to help a marketing team create a\n",
        "description for a retail website of a product based\n",
        "on a technical fact sheet.\n",
        "\n",
        "Write a product description based on the information\n",
        "provided in the technical specifications delimited by\n",
        "triple backticks.\n",
        "Use at most 50 words.\n",
        "\n",
        "Technical specifications: ```{fact_sheet_table}```\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kTf_IDEvPZ8"
      },
      "source": [
        "____________\n",
        "**Iterative Prompt Development:**\n",
        "\n",
        "--> IT IS FOCUSING ON THE WRONG THINGS\n",
        "\n",
        "Ask it to focus on the aspects that are relevant to the intended audience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "WiJZCfshEhGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783c89b8-5ba0-410a-82f8-36bdb6c008e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Introducing our versatile and sleek table, a perfect addition to any modern office space. Constructed with a sturdy metal frame and adjustable leg levelers, this table ensures durability and stability on uneven surfaces. With options for tabletop finish, leg style, and color choices, it can be customized to suit any workspace. Available in various sizes, it is suitable for both collaborative workspaces and individual offices. Choose from high-quality laminate, veneer, or glass tabletops, and enjoy the convenience of multiple color choices. Designed and manufactured in Germany, this table is ideal for home offices or corporate environments.\n"
          ]
        }
      ],
      "source": [
        "#Iterative Prompt Development:\n",
        "#Ask it to focus on the aspects that are relevant to the intended\n",
        "\n",
        "# Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your task is to help a marketing team create a\n",
        "description for a retail website of a product based\n",
        "on a technical fact sheet.\n",
        "\n",
        "Write a product description based on the information\n",
        "provided in the technical specifications delimited by\n",
        "triple backticks.\n",
        "\n",
        "The description is intended for furniture retailers,\n",
        "so should be technical in nature and focus on the\n",
        "materials the product is constructed from.\n",
        "\n",
        "Use at most 50 words.\n",
        "\n",
        "Technical specifications: ```{fact_sheet_table}```\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vljx9K2vUyY"
      },
      "source": [
        "______________\n",
        "Iterative Prompt Development:\n",
        "\n",
        "--> Ask it to extract information and organize it in a table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qdITUXyZE1by",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b92003-928b-4fc3-d0cb-cad934337e0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<div>\n",
            "  <h2>Product Description</h2>\n",
            "  <p>A versatile addition to our contemporary furniture collection, this table complements modern office spaces with its sleek design. Part of a comprehensive range that includes desks, storage units, and ergonomic chairs, this table offers options for tabletop finish, leg style, and color choices. Available in various sizes to accommodate different workspace needs, it is suitable for both collaborative workspaces and individual offices. Whether for home offices or corporate environments, this table is an ideal choice.</p>\n",
            "  <p>The construction of this table ensures durability and stability. The sturdy metal frame with a powder-coated finish provides strength and longevity. Adjustable leg levelers are included to ensure stability on uneven surfaces.</p>\n",
            "  <p>The materials used in this table are of high quality. The frame is made of powder-coated steel, which adds to its durability. The tabletop is available in a choice of high-quality laminate, veneer, or glass, allowing you to customize the look and feel of the table to suit your preferences.</p>\n",
            "  <p>This table is designed and manufactured in Germany, guaranteeing its quality and craftsmanship.</p>\n",
            "  \n",
            "  <h2>Product Dimensions</h2>\n",
            "  <table>\n",
            "    <tr>\n",
            "      <th>Dimension</th>\n",
            "      <th>Measurement (inches)</th>\n",
            "    </tr>\n",
            "    <tr>\n",
            "      <td>Width</td>\n",
            "      <td>47.24\"</td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "      <td>Depth</td>\n",
            "      <td>31.50\"</td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "      <td>Height</td>\n",
            "      <td>29.53\"</td>\n",
            "    </tr>\n",
            "  </table>\n",
            "</div>\n",
            "\n",
            "Product IDs: 120 CM, 80 CM, 75 CM\n"
          ]
        }
      ],
      "source": [
        "#Iterative Prompt Development: --> Ask it to extract information and organize it in a table.\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your task is to help a marketing team create a\n",
        "description for a retail website of a product based\n",
        "on a technical fact sheet.\n",
        "\n",
        "Write a product description based on the information\n",
        "provided in the technical specifications delimited by\n",
        "triple backticks.\n",
        "\n",
        "The description is intended for furniture retailers,\n",
        "so should be technical in nature and focus on the\n",
        "materials the product is constructed from.\n",
        "\n",
        "At the end of the description, include every 7-character\n",
        "Product ID in the technical specification.\n",
        "\n",
        "After the description, include a table that gives the\n",
        "product's dimensions. The table should have two columns.\n",
        "In the first column include the name of the dimension.\n",
        "In the second column include the measurements in inches only.\n",
        "\n",
        "Give the table the title 'Product Dimensions'.\n",
        "\n",
        "Format everything as HTML that can be used in a website.\n",
        "Place the description in a <div> element.\n",
        "\n",
        "Technical specifications: ```{fact_sheet_table}```\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk4Sh9HBvcVf"
      },
      "source": [
        "____________\n",
        "**Iterative Prompt Development:**\n",
        "\n",
        "Let's see that select with HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Q8GjFnJ_FAbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "635cd305-d55e-45f6-e076-a354656aaa8c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div>\n",
              "  <h2>Product Description</h2>\n",
              "  <p>A versatile addition to our contemporary furniture collection, this table complements modern office spaces with its sleek design. Part of a comprehensive range that includes desks, storage units, and ergonomic chairs, this table offers options for tabletop finish, leg style, and color choices. Available in various sizes to accommodate different workspace needs, it is suitable for both collaborative workspaces and individual offices. Whether for home offices or corporate environments, this table is an ideal choice.</p>\n",
              "  <p>The construction of this table ensures durability and stability. The sturdy metal frame with a powder-coated finish provides strength and longevity. Adjustable leg levelers are included to ensure stability on uneven surfaces.</p>\n",
              "  <p>The materials used in this table are of high quality. The frame is made of powder-coated steel, which adds to its durability. The tabletop is available in a choice of high-quality laminate, veneer, or glass, allowing you to customize the look and feel of the table to suit your preferences.</p>\n",
              "  <p>This table is designed and manufactured in Germany, guaranteeing its quality and craftsmanship.</p>\n",
              "  \n",
              "  <h2>Product Dimensions</h2>\n",
              "  <table>\n",
              "    <tr>\n",
              "      <th>Dimension</th>\n",
              "      <th>Measurement (inches)</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Width</td>\n",
              "      <td>47.24\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Depth</td>\n",
              "      <td>31.50\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Height</td>\n",
              "      <td>29.53\"</td>\n",
              "    </tr>\n",
              "  </table>\n",
              "</div>\n",
              "\n",
              "Product IDs: 120 CM, 80 CM, 75 CM"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Iterative Prompt Development: Let's see that select with HTML\n",
        "\n",
        "# Import necessary module for displaying HTML in IPython\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Assuming response contains HTML content\n",
        "html_content = response\n",
        "\n",
        "# Display the HTML content\n",
        "display(HTML(html_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1xgLydnviaQ"
      },
      "source": [
        "________________\n",
        "**Summarizing with the help of the ChatGPT:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "cCCsGag9FKqN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33febcc7-390c-4eff-f75c-9fa1fba0f452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall, I am very satisfied with my purchase of the bunny plush. Despite its smaller size, the plush is incredibly soft and cute, which my niece absolutely adores. The friendly facial expression adds to its charm. However, I do think that for the price point, it could have been slightly larger. I will definitely consider looking for larger options at a similar cost in the future. Additionally, the early delivery of the plush was a pleasant surprise, giving me some extra time to enjoy it before giving it to my niece.\n"
          ]
        }
      ],
      "source": [
        "#Text to summarize\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prod_review = \"\"\"\n",
        "Purchased this adorable bunny plush for my niece's birthday, and she\n",
        "absolutely adores it, carrying it around everywhere. The plush is\n",
        "incredibly soft, and its cuteness factor is off the charts. The facial\n",
        "expression is friendly and inviting. However, I did find it slightly\n",
        "smaller than anticipated for the price point. Considering alternatives,\n",
        "I suspect there might be larger options available at a similar cost.\n",
        "The plush arrived a day earlier than the expected delivery date, allowing\n",
        "me a chance to enjoy some playful moments with it before presenting\n",
        "it to my niece.\n",
        "\"\"\"\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prod_review)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gvBOIlavnHH"
      },
      "source": [
        "______________\n",
        "Summarize with a word/sentence/character limit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "peTixy4SFKnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15520778-93cc-4924-a42a-b5c920067154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The reviewer purchased a bunny plush for their niece's birthday. The plush is soft and cute, but smaller than expected for the price. It arrived early.\n"
          ]
        }
      ],
      "source": [
        "#Summarize with a word/sentence/character limit\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your task is to generate a short summary of a product \\\n",
        "review from an ecommerce site.\n",
        "\n",
        "Summarize the review below, delimited by triple\n",
        "backticks, in at most 30 words.\n",
        "\n",
        "Review: ```{prod_review}```\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52ah273RvruA"
      },
      "source": [
        "_____________\n",
        "Summarize with a focus on shipping and delivery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "qudA-zauFXOB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "696070a0-fdf9-4676-931b-1870077920d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The plush bunny arrived a day earlier than expected, allowing the customer to enjoy it before gifting it. However, they found it slightly smaller than anticipated.\n"
          ]
        }
      ],
      "source": [
        "#Summarize with a focus on shipping and delivery\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your task is to generate a short summary of a product \\\n",
        "review from an ecommerce site to give feedback to the \\\n",
        "Shipping deparmtment.\n",
        "\n",
        "Summarize the review below, delimited by triple\n",
        "backticks, in at most 30 words, and focusing on any aspects \\\n",
        "that mention shipping and delivery of the product.\n",
        "\n",
        "Review: ```{prod_review}```\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUey87zwvwnv"
      },
      "source": [
        "___________\n",
        "Summarize with a focus on price and value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "jx9VdY6QFe7M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2affabe3-9084-4b75-a691-0439d0a19074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The reviewer loved the softness and cuteness of the bunny plush, but found it slightly smaller than expected for the price. They suggest exploring larger options at a similar cost.\n"
          ]
        }
      ],
      "source": [
        "#Summarize with a focus on price and value\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your task is to generate a short summary of a product \\\n",
        "review from an ecommerce site to give feedback to the \\\n",
        "pricing deparmtment, responsible for determining the \\\n",
        "price of the product.\n",
        "\n",
        "Summarize the review below, delimited by triple\n",
        "backticks, in at most 30 words, and focusing on any aspects \\\n",
        "that are relevant to the price and perceived value.\n",
        "\n",
        "Review: ```{prod_review}```\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWA8XxVmv1CA"
      },
      "source": [
        "__________________\n",
        "Try “extract” instead of “summarize”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "904F52HVFjx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd0782e5-7303-43a9-e8d1-9dcd3de421fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The plush arrived a day earlier than the expected delivery date.\n"
          ]
        }
      ],
      "source": [
        "#Try “extract” instead of “summarize”\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Your task is to extract relevant information from \\\n",
        "a product review from an ecommerce site to give \\\n",
        "feedback to the Shipping department.\n",
        "\n",
        "From the review below, delimited by triple quotes \\\n",
        "extract the information relevant to shipping and \\\n",
        "delivery. Limit to 30 words.\n",
        "\n",
        "Review: ```{prod_review}```\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juy4j6hSv6Fw"
      },
      "source": [
        "____________\n",
        "Summarize multiple product reviews :\n",
        "\n",
        "We can use this approach to summaize the longer reviews for our personal usecase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "5xr-ap7TFo9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f52350c8-3681-4e28-ce53-1490f7f916df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Adorable bunny plush loved by niece, soft and cute, but smaller than expected for the price. Early delivery. \n",
            "\n",
            "1 Attractive and reasonably priced desk chair with swift shipping. Responsive customer service and comfortable addition to workspace. \n",
            "\n",
            "2 Upgraded kitchen blender worked well initially but motor started acting up. Disappointed with decline in quality. Quick delivery. \n",
            "\n",
            "3 Fast delivery, responsive customer support, easy assembly, and functional storage make this reading lamp a worthwhile purchase. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Summarize multiple product reviews :\n",
        "\n",
        "# Define the input text reviews\n",
        "\n",
        "review_1 = prod_review\n",
        "\n",
        "review_2 = \"\"\"\n",
        "Needed a new desk chair for my home office and stumbled upon this\n",
        "one with an attractive design and reasonable price. The shipping\n",
        "was swift, arrived within three days. However, upon assembly, I\n",
        "noticed a small tear in the upholstery. Contacted customer support,\n",
        "and they promptly sent a replacement upholstery cover. Putting it\n",
        "together was a breeze, and now it's a comfortable addition to my\n",
        "workspace. Appreciate the responsive customer service and overall\n",
        "pleased with the purchase.\n",
        "\"\"\"\n",
        "\n",
        "review_3 = \"\"\"\n",
        "Decided to upgrade my kitchen blender and opted for this model\n",
        "due to its features and a discounted price of $59 during a sale.\n",
        "It worked well for the first few months, handling various tasks\n",
        "smoothly. However, recently, the motor started acting up, making\n",
        "an odd noise. Unfortunately, the warranty had already expired, so\n",
        "I ended up purchasing a different brand. While it served its\n",
        "purpose, the decline in quality over time was disappointing.\n",
        "On a positive note, the delivery was quick, arriving within two days.\n",
        "\"\"\"\n",
        "\n",
        "review_4 = \"\"\"\n",
        "After my trusted reading lamp broke, I ordered this one for its\n",
        "storage feature and reasonable cost. The delivery was impressively\n",
        "fast, reaching me in just two days. Unfortunately, the lamp's pull\n",
        "string snapped during transit. Contacted customer support, and they\n",
        "promptly shipped a replacement, arriving within a few days. Assembly\n",
        "was straightforward, and the lamp now stands gracefully in my bedroom.\n",
        "This company truly cares about its customers, making\n",
        "it a worthwhile purchase.\n",
        "\"\"\"\n",
        "\n",
        "reviews = [review_1, review_2, review_3, review_4]\n",
        "\n",
        "# Construct the prompt with the provided text\n",
        "for i in range(len(reviews)):\n",
        "    prompt = f\"\"\"\n",
        "    Your task is to generate a short summary of a product \\\n",
        "    review from an ecommerce site.\n",
        "\n",
        "    Summarize the review below, delimited by triple \\\n",
        "    backticks in at most 20 words.\n",
        "\n",
        "    Review: ```{reviews[i]}```\n",
        "    \"\"\"\n",
        "    # Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "    response = get_completion(prompt)\n",
        "\n",
        "    # Print the review index, summary, and a newline\n",
        "    print(i, response, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsdoq-YYv-dm"
      },
      "source": [
        "_____________\n",
        "**Inference with the Large Language Models:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Ky6VfxBqFvx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3535a33e-338f-40a5-cfd3-734b78c1d48c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm glad to hear that you had a positive experience with your new blender purchase. It's unfortunate that there was a crack in the jug, but it's great that the customer support team was able to swiftly send you a replacement. It's always reassuring when a company is responsive and efficient in resolving issues. It's also good to know that putting the blender together was easy and that it has been working seamlessly since. Overall, it sounds like you made a good choice with your purchase and that you're satisfied with both the product and the company's customer service.\n"
          ]
        }
      ],
      "source": [
        "#Inference with the Large Language Models:\n",
        "#Product review text\n",
        "\n",
        "blender_review = \"\"\"\n",
        "Recently purchased a new blender for my kitchen, and I was drawn\n",
        "to this model due to its sleek design and reasonable cost. The\n",
        "delivery was surprisingly quick, arriving within three days.\n",
        "However, upon unboxing, I noticed a small crack in the blender\n",
        "jug. I contacted customer support, and they swiftly sent a\n",
        "replacement, which arrived within a few days. Putting the\n",
        "blender together was a breeze, and it has been working\n",
        "seamlessly since. The customer service was commendable,\n",
        "and I'm satisfied with both the product and the company's\n",
        "responsiveness.\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(blender_review)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccriEJCCwFAQ"
      },
      "source": [
        "____________\n",
        "Sentiment (positive/negative)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "QSsCivInF3d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd279d3a-adce-4cd2-d362-f11e8bf56317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment of the product review is positive.\n"
          ]
        }
      ],
      "source": [
        "#Inference with the Large Language Models:\n",
        "\n",
        "#Sentiment (positive/negative)\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "What is the sentiment of the following product review,\n",
        "which is delimited with triple backticks?\n",
        "\n",
        "Review text: '''{blender_review}'''\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN8IoZSEwL5-"
      },
      "source": [
        "---> case 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "-YQ0obIiF8qN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ce4517-7bd5-43a0-9a65-1ad37a0a9a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ],
      "source": [
        "#Inference with the Large Language Models:\n",
        "\n",
        "#---> case 2\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "What is the sentiment of the following product review,\n",
        "which is delimited with triple backticks?\n",
        "\n",
        "Give your answer as a single word, either \"positive\" \\\n",
        "or \"negative\".\n",
        "\n",
        "Review text: '''{blender_review}'''\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPNX-K4OwO-V"
      },
      "source": [
        "____________\n",
        "Identify types of emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Tid_zXXfGAfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09802689-10db-47cf-d86b-83b31de02171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drawn, surprised, noticed, satisfied, commendable\n"
          ]
        }
      ],
      "source": [
        "#Identify types of emotions\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Identify a list of emotions that the writer of the \\\n",
        "following review is expressing. Include no more than \\\n",
        "five items in the list. Format your answer as a list of \\\n",
        "lower-case words separated by commas.\n",
        "\n",
        "Review text: '''{blender_review}'''\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEhzw0wewjKt"
      },
      "source": [
        "_____________\n",
        "Identify anger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Zc6xYk5QGCSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492c5ecc-03b1-4873-fa3d-13f45d3218fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No\n"
          ]
        }
      ],
      "source": [
        "#Identify anger\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Is the writer of the following review expressing anger?\\\n",
        "The review is delimited with triple backticks. \\\n",
        "Give your answer as either yes or no.\n",
        "\n",
        "Review text: '''{blender_review}'''\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewAI4ONFwpMA"
      },
      "source": [
        "_____________\n",
        "Extract product and company name from customer reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "NiRkzFEXGJ1H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae51c315-7be2-497b-9d3d-74f64b524ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"Item\": \"blender\",\n",
            "  \"Brand\": \"unknown\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "#Extract product and company name from customer reviews\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Identify the following items from the review text:\n",
        "- Item purchased by reviewer\n",
        "- Company that made the item\n",
        "\n",
        "The review is delimited with triple backticks. \\\n",
        "Format your response as a JSON object with \\\n",
        "\"Item\" and \"Brand\" as the keys.\n",
        "If the information isn't present, use \"unknown\" \\\n",
        "as the value.\n",
        "Make your response as short as possible.\n",
        "\n",
        "Review text: '''{blender_review}'''\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9YybhcQwv4m"
      },
      "source": [
        "_______________\n",
        "Doing multiple tasks at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "rLrHNaeOGUzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3a81db-d7fc-40a9-9ab1-745e62dc5f3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"Sentiment\": \"positive\",\n",
            "  \"Anger\": false,\n",
            "  \"Item\": \"blender\",\n",
            "  \"Brand\": \"unknown\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "#Doing multiple tasks at once\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Identify the following items from the review text:\n",
        "- Sentiment (positive or negative)\n",
        "- Is the reviewer expressing anger? (true or false)\n",
        "- Item purchased by reviewer\n",
        "- Company that made the item\n",
        "\n",
        "The review is delimited with triple backticks. \\\n",
        "Format your response as a JSON object with \\\n",
        "\"Sentiment\", \"Anger\", \"Item\" and \"Brand\" as the keys.\n",
        "If the information isn't present, use \"unknown\" \\\n",
        "as the value.\n",
        "Make your response as short as possible.\n",
        "Format the Anger value as a boolean.\n",
        "\n",
        "Review text: '''{blender_review}'''\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjxR-FOMw1K8"
      },
      "source": [
        "_______________\n",
        "Inferring topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Pp8nE3m6tx7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe27c1e-1c64-4d0c-dcc7-f7d0e83b4249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ISRO, satisfaction, Priya Patel, survey, Public Services.\n"
          ]
        }
      ],
      "source": [
        "# The story you provided\n",
        "story = \"\"\"\n",
        "In a recent government-conducted survey, public sector employees\n",
        "were asked to assess their satisfaction levels within the departments\n",
        "they serve. The results unveiled that the Indian Space Research\n",
        "Organisation (ISRO) emerged as the most favored department, boasting\n",
        "an impressive satisfaction rating of 92%.\n",
        "\n",
        "One ISRO employee, Priya Patel, shared her thoughts on the survey,\n",
        "saying, \"I'm elated but not surprised by ISRO's high satisfaction\n",
        "rating. Working here is more than just a job; it's a journey filled\n",
        "with challenges, growth, and a shared passion for space exploration.\n",
        "Being part of ISRO is a source of immense pride for me.\"\n",
        "\n",
        "The survey results were met with enthusiasm from ISRO's leadership,\n",
        "with Director Ravi Kapoor expressing, \"We are delighted that our\n",
        "employees find satisfaction in their roles at ISRO. Our success is\n",
        "a testament to the hard work and dedication of our team. ISRO\n",
        "continues to push boundaries in space exploration, and this positive\n",
        "feedback from our employees fuels our commitment to excellence.\"\n",
        "\n",
        "Contrastingly, the survey indicated that the Department of Public\n",
        "Services had the lowest satisfaction rating, with only 48% of its\n",
        "employees expressing contentment with their positions. The government\n",
        "has acknowledged these concerns and is committed to addressing them,\n",
        "emphasizing a dedication to enhancing job satisfaction across all\n",
        "public sector departments.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt to infer five topics from the given text\n",
        "prompt = f\"\"\"\n",
        "Determine five topics that are being discussed in the\n",
        "following text, which is delimited by triple backticks.\n",
        "\n",
        "Make each item one or two words long.\n",
        "\n",
        "Format your response as a list of items separated by commas.\n",
        "\n",
        "Text sample: '''{story}'''\n",
        "\"\"\"\n",
        "\n",
        "# Call OpenAI API to get the completion\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",  # You may need to adjust the engine based on the available models\n",
        "    prompt=prompt,\n",
        "    max_tokens=100  # You can adjust this parameter based on the desired response length\n",
        ")\n",
        "\n",
        "# Extract and print the generated topics\n",
        "topic_list = response['choices'][0]['text'].strip()\n",
        "print(topic_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ1ghp_DxMIm"
      },
      "source": [
        "__________\n",
        "Prompt to determine whether each item in the given list of topics is discussed in the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "7UlKWzHCm_qz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95999be6-c1b7-40a4-fc7f-c34caa56cb37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 0, 0, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "# Prompt to determine whether each item in the given list of topics is discussed in the text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Determine whether each item in the following list of \\\n",
        "topics is a topic in the text below, which\n",
        "is delimited with triple backticks.\n",
        "\n",
        "Give your answer as list with 0 or 1 for each topic.\\\n",
        "\n",
        "List of topics: ISRO, USA, engineering, space exploration, public services\n",
        "\n",
        "Text sample: '''{story}'''\n",
        "\"\"\"\n",
        "\n",
        "# Call OpenAI API to get the completion\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg55M8duxXtE"
      },
      "source": [
        "_____________\n",
        "Prompt to format the given list as a Python list with the name 'topic_list'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "-pfcBYI4n-qi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3621761-3ee3-4fa6-8e73-5f4def8e62b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic_list = ['ISRO', 'USA', 'engineering', 'space exploration', 'public services']\n"
          ]
        }
      ],
      "source": [
        "# Prompt to format the given list as a Python list with the name 'topic_list'\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Format the list below as a python list with name: topic_lit\n",
        "\n",
        "\n",
        "List of topics: ISRO, USA, engineering, space exploration, public services\n",
        "\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpo311xExjza"
      },
      "source": [
        "_____________\n",
        "This code takes a provided list of binary results (`result`) and a corresponding list of topics (`topic_list`).\n",
        "\n",
        "It formats these lists into a readable output by combining each topic with its corresponding binary value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "KsLX855SvWXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da30b40-0326-456c-a544-ed3ec8462ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ISRO: 1\n",
            "USA: 0\n",
            "engineering: 0\n",
            "space exploration: 1\n",
            "public services: 1\n"
          ]
        }
      ],
      "source": [
        "# Provided result\n",
        "result = [1, 0, 0, 1, 1]\n",
        "\n",
        "# List of topics\n",
        "topic_list = ['ISRO', 'USA', 'engineering', 'space exploration', 'public services']\n",
        "\n",
        "# Output in the desired format\n",
        "formatted_output = \"\\n\".join([f\"{topic}: {value}\" for topic, value in zip(topic_list, result)])\n",
        "\n",
        "# Print the formatted output\n",
        "print(formatted_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYKfx8Wux4qD"
      },
      "source": [
        "___________\n",
        "Make a news alert for certain topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Te-Zimcbxg8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3f773d4-a83c-4df6-dfc2-487ee345cfc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALERT: New ISRO story!\n"
          ]
        }
      ],
      "source": [
        "# Assuming 'response' is the string containing the generated completion\n",
        "response = \"\"\"\n",
        "ISRO: 1\n",
        "USA: 0\n",
        "engineering: 0\n",
        "space exploration: 1\n",
        "public services: 1\n",
        "\"\"\"\n",
        "\n",
        "# Create topic_dict from the response\n",
        "topic_dict = {i.split(': ')[0].lower(): int(i.split(': ')[1]) for i in response.split('\\n') if ': ' in i}\n",
        "\n",
        "# Check if there is a new ISRO story\n",
        "if topic_dict.get('isro', 0) == 1:\n",
        "    print(\"ALERT: New ISRO story!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxnL2DfyyE4N"
      },
      "source": [
        "__________\n",
        "**Transforming**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "8WFEvDrpGm8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41b3eb50-c976-4879-f6e8-af201d4ab7f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hola, me gustaría pedir una pizza.\n"
          ]
        }
      ],
      "source": [
        "#Transforming\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Translate the following English text to Spanish: \\\n",
        "```Hi, I would like to order a pizza```\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmKpzt4ZyP1y"
      },
      "source": [
        "_____________\n",
        "Transforming case 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "QmE2Qs-NGr6u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66d7f4ac-548d-4275-9795-e3b84b633651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This language is French.\n"
          ]
        }
      ],
      "source": [
        "#Transforming case 2\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Tell me which language this is:\n",
        "```Combien coûte le vélo?```\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKmP5jrDycCN"
      },
      "source": [
        "___________\n",
        "Transforming case 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ap515lhGGveO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1fe1f79-cbe5-4fc0-9292-99c4d06d2c6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "French: Je veux commander un café\n",
            "Spanish: Quiero pedir un café\n",
            "English: I want to order a coffee\n"
          ]
        }
      ],
      "source": [
        "#Transforming case 3\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Translate the following  text to French and Spanish\n",
        "and English pirate: \\\n",
        "```I want to order a Cofee```\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvH6Jq7gyet2"
      },
      "source": [
        "______________\n",
        "Transforming case 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "JwG7YLQyGzQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42f00b0a-d30c-4b25-dbb1-fab3312d1500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formal: ¿Le gustaría ordenar un té?\n",
            "Informal: ¿Te gustaría ordenar un té?\n"
          ]
        }
      ],
      "source": [
        "#Transforming case 4\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Translate the following text to Spanish in both the \\\n",
        "formal and informal forms:\n",
        "'Would you like to order a Tea?'\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAXbJPb1yygv"
      },
      "source": [
        "___________\n",
        "**Universal Translator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "CjxMAHhyn33o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ae6cad2-49a9-4dcb-945c-25d0aae14334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original message (This language is French.): Les performances du système sont meilleures maintenant.\n",
            "The performance of the system is better now. (English)\n",
            "\n",
            "시스템의 성능이 지금은 더 좋아졌습니다. (Korean) \n",
            "\n",
            "Original message (The language is Spanish.): Mi monitor tiene algunas manchas oscuras.\n",
            "My monitor has some dark spots.\n",
            "\n",
            "나의 모니터에는 어두운 자국이 있습니다. \n",
            "\n",
            "Original message (The language is Italian.): Il mio mouse non clicca.\n",
            "English: \"My mouse doesn't click.\"\n",
            "Korean: \"내 마우스는 클릭하지 않습니다.\" \n",
            "\n",
            "Original message (The language is Polish.): Mój klawisz enter w klawiaturze jest zepsuty.\n",
            "English: \"My enter key on the keyboard is broken.\"\n",
            "\n",
            "Korean: \"내 키보드의 엔터 키가 고장 났어요.\" \n",
            "\n",
            "Original message (The language is Chinese.): 我的屏幕没有开启.\n",
            "English: \"My screen is not turned on.\"\n",
            "\n",
            "Korean: \"내 화면이 켜지지 않았어요.\" \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Universal Translator\n",
        "\n",
        "# Define the input text\n",
        "\n",
        "user_messages = [\n",
        "  \"Les performances du système sont meilleures maintenant.\",  # The system performance is better now.\n",
        "  \"Mi monitor tiene algunas manchas oscuras.\",                # My monitor has some dark patches.\n",
        "  \"Il mio mouse non clicca.\",                                 # My mouse is not clicking.\n",
        "  \"Mój klawisz enter w klawiaturze jest zepsuty.\",            # The enter key on my keyboard is broken.\n",
        "  \"我的屏幕没有开启.\"                                          # My screen is not turning on.\n",
        "]\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "\n",
        "for issue in user_messages:\n",
        "    prompt = f\"Tell me what language this is: ```{issue}```\"\n",
        "    lang = get_completion(prompt)\n",
        "    print(f\"Original message ({lang}): {issue}\")\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Translate the following  text to English \\\n",
        "    and Korean: ```{issue}```\n",
        "    \"\"\"\n",
        "    response = get_completion(prompt)\n",
        "    print(response, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gwd63RAzBC2"
      },
      "source": [
        "____________\n",
        "Tone Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Awt5-ViPG8ri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fb20e6e-29e8-4a07-f9af-e21d3ac29671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear Sir/Madam,\n",
            "\n",
            "I hope this letter finds you well. My name is John, and I am writing to bring your attention to the specifications of a laptop that I believe may be of interest to you.\n",
            "\n",
            "Thank you for taking the time to review this information.\n",
            "\n",
            "Yours sincerely,\n",
            "John\n"
          ]
        }
      ],
      "source": [
        "#Tone Transformation\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Translate the following from slang to a business letter:\n",
        "'Dude, This is John, check out this spec on this laptop.'\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate a model completion based on the provided prompt.\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the generated completion\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xhi5a5ezOKf"
      },
      "source": [
        "___________\n",
        "\n",
        "**Format Conversion**\n",
        "\n",
        "ChatGPT can translate between formats.\n",
        "\n",
        "The prompt should describe the input and output formats.\n",
        "\n",
        "JSON to HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "UUFCsIhHHCZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d06b1e70-a066-4bca-eb17-d7d94ce372a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<table border='1'>\n",
            "<caption>Restaurant Employees</caption>\n",
            "<tr><th>Name</th><th>Email</th></tr>\n",
            "<tr><td>Ravi</td><td>ravirao@gmail.com</td></tr>\n",
            "<tr><td>Sam</td><td>sam32@gmail.com</td></tr>\n",
            "<tr><td>Nikita</td><td>nikita2023@gmail.com</td></tr>\n",
            "</table>\n"
          ]
        }
      ],
      "source": [
        "#JSON to HTML\n",
        "\n",
        "# Input Data\n",
        "data_json = {\n",
        "    \"hotel employees\": [\n",
        "        {\"name\": \"Ravi\", \"email\": \"ravirao@gmail.com\"},\n",
        "        {\"name\": \"Sam\", \"email\": \"sam32@gmail.com\"},\n",
        "        {\"name\": \"Nikita\", \"email\": \"nikita2023@gmail.com\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Construct an HTML table directly from the dictionary with a caption\n",
        "html_table = \"<table border='1'>\\n\"  # Open the table tag with a border attribute\n",
        "html_table += \"<caption>Restaurant Employees</caption>\\n\"  # Add a caption for the table\n",
        "html_table += \"<tr><th>Name</th><th>Email</th></tr>\\n\"  # Add table headers\n",
        "\n",
        "# Iterate through each employee and add a table row for each\n",
        "for employee in data_json[\"hotel employees\"]:\n",
        "    html_table += f\"<tr><td>{employee['name']}</td><td>{employee['email']}</td></tr>\\n\"\n",
        "\n",
        "html_table += \"</table>\"  # Close the table tag\n",
        "\n",
        "# Print the generated HTML table\n",
        "print(html_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucpaisIbzYv2"
      },
      "source": [
        "______________\n",
        "Table format of HTML code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "UcQYYql-HEeL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "6885fda1-625f-48ae-fe8d-28ed0c0b34c5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<caption>Hotel Employees</caption>\n",
              "<tr><th>Name</th><th>Email</th></tr>\n",
              "<tr><td>Ravi</td><td>ravirao@gmail.com</td></tr>\n",
              "<tr><td>Sam</td><td>sam32@gmail.com</td></tr>\n",
              "<tr><td>Nikita</td><td>nikita2023@gmail.com</td></tr>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Table display\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Input Data\n",
        "data_json = {\n",
        "    \"hotel employees\": [\n",
        "        {\"name\": \"Ravi\", \"email\": \"ravirao@gmail.com\"},\n",
        "        {\"name\": \"Sam\", \"email\": \"sam32@gmail.com\"},\n",
        "        {\"name\": \"Nikita\", \"email\": \"nikita2023@gmail.com\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Construct an HTML table directly from the dictionary with a caption\n",
        "html_table = \"<table border='1'>\\n\"\n",
        "html_table += \"<caption>Hotel Employees</caption>\\n\"\n",
        "html_table += \"<tr><th>Name</th><th>Email</th></tr>\\n\"\n",
        "\n",
        "for employee in data_json[\"hotel employees\"]:\n",
        "    html_table += f\"<tr><td>{employee['name']}</td><td>{employee['email']}</td></tr>\\n\"\n",
        "\n",
        "html_table += \"</table>\"\n",
        "\n",
        "# Display the HTML table\n",
        "display(HTML(html_table))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noLs5fZmzjrt"
      },
      "source": [
        "______________\n",
        "**Spellcheck/Grammar check.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "OVFzQzvxHMmE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd952bdf-4636-40ee-f2eb-28699300a03b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The girl with the white and black puppies has a ball.\n",
            "No errors found.\n",
            "No errors found.\n",
            "Their goes my money. There going to bring they’re bags.\n",
            "\n",
            "No errors found.\n",
            "\n",
            "Rewritten version:\n",
            "Their goes my money. There going to bring their bags.\n",
            "You're going to need your diary.\n",
            "That wine affects my ability to sleep. Have you heard of the butterfly effect?\n",
            "No errors found.\n"
          ]
        }
      ],
      "source": [
        "#Spellcheck/Grammar check.\n",
        "\n",
        "# Define a list of sentences for proofreading\n",
        "text = [\n",
        "    \"The girl with the white and black puppies have a ball.\",  # The girl has a ball.\n",
        "    \"Vasundhara has her notebook.\",  # ok\n",
        "    \"Its going to be a long night. Does the bike need it’s oil changed?\",  # Homonyms\n",
        "    \"Their goes my money. There going to bring they’re bags.\",  # Homonyms\n",
        "    \"Your going to need you’re diary.\",  # Homonyms\n",
        "    \"That wine effects my ability to sleep. Have you heard of the butterfly affect?\",  # Homonyms\n",
        "    \"This text is to cherck chatGPT for speling abilitty\"  # spelling\n",
        "]\n",
        "\n",
        "# Iterate through each sentence in the list\n",
        "for t in text:\n",
        "    # Construct a prompt for proofreading and correction\n",
        "    prompt = f\"\"\"Proofread and correct the following text\n",
        "    and rewrite the corrected version. If you don't find\n",
        "    any errors, just say \"No errors found\". Don't use\n",
        "    any punctuation around the text:\n",
        "    ```{t}```\"\"\"\n",
        "\n",
        "    # Use the get_completion function to generate corrections\n",
        "    response = get_completion(prompt)\n",
        "\n",
        "    # Print the response\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhisaYygzzz_"
      },
      "source": [
        "____________\n",
        "Spellcheck/Grammar check. Example 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "wV6GiirXHVG0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d31ad2d2-1cd8-4718-9fe3-32aa6d34a022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got this for my son for his birthday because he keeps taking mine from my room. Yes, men also like pandas too. He takes it everywhere with him, and it's super soft and cute. However, one of the ears is a bit lower than the other, and I don't think that was intended to be asymmetrical. Additionally, it's a bit smaller than what I paid for. I believe there might be other options available that are bigger for the same price. On the positive side, it arrived a day earlier than expected, so I had the chance to play with it myself before giving it to my son.\n"
          ]
        }
      ],
      "source": [
        "#Spellcheck/Grammar check. Example 2\n",
        "\n",
        "# Define a list of sentences for proofreading\n",
        "\n",
        "text = f\"\"\"\n",
        "Got this for my son for her birthday cuz she keeps taking \\\n",
        "mine from my room.  Yes, men also like pandas too.  She take \\\n",
        "it everywhere with her, and it's super soft and cute.  One of the \\\n",
        "ears is a bit lower than the other, and I don't think that was \\\n",
        "made to be asymmetrical. It's a bit smaller for what I paid for it \\\n",
        "though. I think there might be another options that are bigger for \\\n",
        "the same price.  It arrived a day earlier than expected, so I got \\\n",
        "to play with it myself before I gave it to my son.\n",
        "\"\"\"\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"proofread and correct this review: ```{text}```\"\n",
        "\n",
        "# Use the get_completion function to generate corrections\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the response\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmMzPyGGz_Se"
      },
      "source": [
        "__________\n",
        "Install required redlines Python package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "SyxmcVsRHddm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "111f3bfb-0a99-455d-f5b2-d26abf6a3fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: redlines in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from redlines) (8.1.7)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.3.5 in /usr/local/lib/python3.10/dist-packages (from redlines) (13.7.0)\n",
            "Requirement already satisfied: rich-click<2.0.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from redlines) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.3.5->redlines) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.3.5->redlines) (2.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from rich-click<2.0.0,>=1.6.1->redlines) (4.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.3.5->redlines) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install redlines # Install the 'redlines' Python package using pip."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIpX0cCx0K0-"
      },
      "source": [
        "__________\n",
        "Spellcheck/Grammar check. Example 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "zsRZV173HirR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "8e063c08-19bb-4802-f96a-00fb7280eb1f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Got this for my son for <span style='color:red;font-weight:700;text-decoration:line-through;'>her </span><span style='color:green;font-weight:700;'>his </span>birthday <span style='color:red;font-weight:700;text-decoration:line-through;'>cuz she </span><span style='color:green;font-weight:700;'>because he </span>keeps taking mine from my <span style='color:red;font-weight:700;text-decoration:line-through;'>room.  </span><span style='color:green;font-weight:700;'>room. </span>Yes, men also like pandas <span style='color:red;font-weight:700;text-decoration:line-through;'>too.  She take </span><span style='color:green;font-weight:700;'>too. He takes </span>it everywhere with <span style='color:red;font-weight:700;text-decoration:line-through;'>her, </span><span style='color:green;font-weight:700;'>him, </span>and it's super soft and <span style='color:red;font-weight:700;text-decoration:line-through;'>cute.  One </span><span style='color:green;font-weight:700;'>cute. However, one </span>of the ears is a bit lower than the other, and I don't think that was <span style='color:red;font-weight:700;text-decoration:line-through;'>made </span><span style='color:green;font-weight:700;'>intended </span>to be asymmetrical. <span style='color:red;font-weight:700;text-decoration:line-through;'>It's </span><span style='color:green;font-weight:700;'>Additionally, it's </span>a bit smaller <span style='color:red;font-weight:700;text-decoration:line-through;'>for </span><span style='color:green;font-weight:700;'>than </span>what I paid <span style='color:red;font-weight:700;text-decoration:line-through;'>for it though. </span><span style='color:green;font-weight:700;'>for. </span>I <span style='color:red;font-weight:700;text-decoration:line-through;'>think </span><span style='color:green;font-weight:700;'>believe </span>there might be <span style='color:red;font-weight:700;text-decoration:line-through;'>another </span><span style='color:green;font-weight:700;'>other </span>options <span style='color:green;font-weight:700;'>available </span>that are bigger for the same <span style='color:red;font-weight:700;text-decoration:line-through;'>price.  It </span><span style='color:green;font-weight:700;'>price. On the positive side, it </span>arrived a day earlier than expected, so I <span style='color:red;font-weight:700;text-decoration:line-through;'>got </span><span style='color:green;font-weight:700;'>had the chance </span>to play with it myself before <span style='color:red;font-weight:700;text-decoration:line-through;'>I gave </span><span style='color:green;font-weight:700;'>giving </span>it to my son."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Got this for my son for <span style='color:red;font-weight:700;text-decoration:line-through;'>her </span><span style='color:green;font-weight:700;'>his </span>birthday <span style='color:red;font-weight:700;text-decoration:line-through;'>cuz she </span><span style='color:green;font-weight:700;'>because he </span>keeps taking mine from my <span style='color:red;font-weight:700;text-decoration:line-through;'>room.  </span><span style='color:green;font-weight:700;'>room. </span>Yes, men also like pandas <span style='color:red;font-weight:700;text-decoration:line-through;'>too.  She take </span><span style='color:green;font-weight:700;'>too. He takes </span>it everywhere with <span style='color:red;font-weight:700;text-decoration:line-through;'>her, </span><span style='color:green;font-weight:700;'>him, </span>and it's super soft and <span style='color:red;font-weight:700;text-decoration:line-through;'>cute.  One </span><span style='color:green;font-weight:700;'>cute. However, one </span>of the ears is a bit lower than the other, and I don't think that was <span style='color:red;font-weight:700;text-decoration:line-through;'>made </span><span style='color:green;font-weight:700;'>intended </span>to be asymmetrical. <span style='color:red;font-weight:700;text-decoration:line-through;'>It's </span><span style='color:green;font-weight:700;'>Additionally, it's </span>a bit smaller <span style='color:red;font-weight:700;text-decoration:line-through;'>for </span><span style='color:green;font-weight:700;'>than </span>what I paid <span style='color:red;font-weight:700;text-decoration:line-through;'>for it though. </span><span style='color:green;font-weight:700;'>for. </span>I <span style='color:red;font-weight:700;text-decoration:line-through;'>think </span><span style='color:green;font-weight:700;'>believe </span>there might be <span style='color:red;font-weight:700;text-decoration:line-through;'>another </span><span style='color:green;font-weight:700;'>other </span>options <span style='color:green;font-weight:700;'>available </span>that are bigger for the same <span style='color:red;font-weight:700;text-decoration:line-through;'>price.  It </span><span style='color:green;font-weight:700;'>price. On the positive side, it </span>arrived a day earlier than expected, so I <span style='color:red;font-weight:700;text-decoration:line-through;'>got </span><span style='color:green;font-weight:700;'>had the chance </span>to play with it myself before <span style='color:red;font-weight:700;text-decoration:line-through;'>I gave </span><span style='color:green;font-weight:700;'>giving </span>it to my son."
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Define the input text\n",
        "\n",
        "#Spellcheck/Grammar check. Example 3\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"proofread and correct this review: ```{text}```\"\n",
        "\n",
        "from redlines import Redlines\n",
        "\n",
        "from IPython.display import Markdown\n",
        "\n",
        "diff = Redlines(text, response)\n",
        "display(Markdown(diff.output_markdown))\n",
        "\n",
        "diff = Redlines(text,response)\n",
        "display(Markdown(diff.output_markdown))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPQsw-LN0V3F"
      },
      "source": [
        "____________\n",
        "**Expanding the use case of LLMs**\n",
        "\n",
        "Customize the automated reply to a customer email"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "vdWh2vLJH8je",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ab7905-030a-4d5e-ba7c-4fd481e0dcb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear Valued Customer,\n",
            "\n",
            "Thank you for taking the time to share your review of our iron machine. We appreciate your feedback and apologize for any inconvenience you have experienced.\n",
            "\n",
            "We are sorry to hear about the issues you encountered with the pricing and the build quality of the iron machine. We strive to provide transparent pricing and high-quality products, and we apologize if we fell short in this instance. We understand your concerns regarding the fluctuating pricing and the declining build quality, and we take your feedback seriously.\n",
            "\n",
            "We would like to assure you that customer satisfaction is our top priority. If you have any further concerns or if there is anything we can do to assist you, please do not hesitate to reach out to our customer service team. They will be more than happy to address any issues you may have and provide you with the necessary support.\n",
            "\n",
            "Once again, we apologize for any inconvenience caused and appreciate your valuable feedback. We will take your comments into consideration as we continue to improve our products and services.\n",
            "\n",
            "Thank you for choosing our brand, and we hope to have the opportunity to serve you better in the future.\n",
            "\n",
            "Best regards,\n",
            "\n",
            "AI customer agent\n"
          ]
        }
      ],
      "source": [
        "#Expanding the use case of LLMs\n",
        "#Customize the automated reply to a customer email\n",
        "\n",
        "# given the sentiment from the lesson on \"inferring\",\n",
        "# and the original customer message, customize the email\n",
        "sentiment = \"negative\"\n",
        "\n",
        "# Define the input text\n",
        "\n",
        "# review for a Iron Machine\n",
        "review = f\"\"\"\n",
        "I recently purchased the iron machine during a Black Friday\n",
        "sale, and initially, the deal seemed quite attractive at $39,\n",
        "marked down from $59. However, I noticed a peculiar trend in\n",
        "pricing. Just a couple of weeks after my purchase, the same\n",
        "iron machine was listed at a regular price of $49, making me\n",
        "question the transparency of the pricing strategy.\n",
        "\n",
        "Upon unboxing, the design looked sleek and modern, but upon\n",
        "closer inspection, I noticed some flimsy components, particularly\n",
        "the water reservoir cap, which seemed prone to leakage. The iron's\n",
        "steam function worked well initially, providing a crisp finish to\n",
        "my clothes. However, after a few weeks of regular use, the steam\n",
        "output seemed to weaken.\n",
        "\n",
        "I follow a careful ironing routine, primarily using the machine for\n",
        "delicate fabrics, and it did the job adequately. Yet, I can't help\n",
        "but feel that the overall build quality has regressed compared to\n",
        "previous models I've owned. The iron plate, in particular, seems\n",
        "less durable.\n",
        "\n",
        "One positive aspect is the quick heating feature, allowing me to\n",
        "start ironing within a minute of turning it on. The cord length\n",
        "is also convenient, providing flexibility while navigating\n",
        "through garments.\n",
        "\n",
        "I did encounter a slight hiccup with the temperature control\n",
        "knob, which became somewhat stiff after a few weeks. It didn't\n",
        "affect the functionality, but it was an inconvenience.\n",
        "\n",
        "In terms of longevity, my previous iron lasted for several\n",
        "years, so the fact that I noticed these issues within a few\n",
        "months of use is concerning. Customer service was responsive\n",
        "when I reached out about the temperature control issue, but\n",
        "unfortunately, the warranty had already expired.\n",
        "\n",
        "In conclusion, while the iron machine gets the job done\n",
        "for everyday use, the declining build quality and fluctuating\n",
        "pricing make me question its long-term reliability. It seems\n",
        "like the brand is relying on its established name rather than\n",
        "consistently delivering top-notch products. Delivery was prompt,\n",
        "arriving within three days.\n",
        "\"\"\"\n",
        "\n",
        "#Construct the prompt with the provided text\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You are a customer service AI assistant.\n",
        "Your task is to send an email reply to a valued customer.\n",
        "Given the customer email delimited by ```, \\\n",
        "Generate a reply to thank the customer for their review.\n",
        "If the sentiment is positive or neutral, thank them for \\\n",
        "their review.\n",
        "If the sentiment is negative, apologize and suggest that \\\n",
        "they can reach out to customer service.\n",
        "Make sure to use specific details from the review.\n",
        "Write in a concise and professional tone.\n",
        "Sign the email as `AI customer agent`.\n",
        "Customer review: ```{review}```\n",
        "Review sentiment: {sentiment}\n",
        "\"\"\"\n",
        "\n",
        "# Use the get_completion function to generate corrections\n",
        "response = get_completion(prompt)\n",
        "\n",
        "# Print the response\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}